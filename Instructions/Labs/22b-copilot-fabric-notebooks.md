---
lab:
  title: "An√°lisis de datos con Apache\_Spark y Copilot en cuadernos de Microsoft\_Fabric"
  module: Get started with Copilot in Fabric for data engineering
---

# An√°lisis de datos con Apache¬†Spark y Copilot en cuadernos de Microsoft¬†Fabric

En este laboratorio, se usa Copilot para Ingenier√≠a de datos de Fabric para cargar, transformar y guardar datos en un almac√©n de lago de datos mediante un cuaderno. Los cuadernos proporcionan un entorno interactivo que combina c√≥digo, visualizaciones y texto narrativo en un √∫nico documento. Este formato facilita la documentaci√≥n del flujo de trabajo, explicar el razonamiento y compartir resultados con otros usuarios. Al usar cuadernos, puede desarrollar y probar c√≥digo de forma iterativa, visualizar los datos en cada paso y mantener un registro claro del proceso de an√°lisis. Este enfoque mejora la colaboraci√≥n, la reproducibilidad y la comprensi√≥n, lo que hace que los cuadernos sean una herramienta ideal para las tareas de ingenier√≠a y an√°lisis de datos.

Tradicionalmente, para trabajar con cuadernos para la ingenier√≠a de datos es necesario escribir c√≥digo en lenguajes como Python o Scala, y tener un conocimiento s√≥lido de marcos y bibliotecas como Apache¬†Spark y pandas. Esto puede ser dif√≠cil para aquellos que no est√°n familiarizados con estas herramientas ni con la programaci√≥n. Con Copilot en cuadernos de Fabric, puede describir las tareas de datos en lenguaje natural y Copilot generar√° el c√≥digo que necesita y controlar√° gran parte de la complejidad t√©cnica, para que pueda centrarse en el an√°lisis.

Este ejercicio deber√≠a tardar en completarse **30** minutos aproximadamente.

## Temas que se abordar√°n

Despu√©s de completar este laboratorio, podr√°:

- Crear y configurar un √°rea de trabajo de Microsoft¬†Fabric y un almac√©n de lago de datos para las tareas de ingenier√≠a de datos.
- Usar Copilot en cuadernos de Fabric para generar c√≥digo a partir de mensajes en lenguaje natural.
- Ingerir, limpiar y transformar datos mediante flujos de trabajo asistidos por Apache¬†Spark y Copilot.
- Normalizar y preparar conjuntos de datos estad√≠sticos para su an√°lisis mediante la divisi√≥n, el filtrado y la conversi√≥n de tipos de datos.
- Guardar los datos transformados como una tabla en el almac√©n de lago de datos para el an√°lisis de bajada.
- Usar Copilot para generar consultas y visualizaciones para la exploraci√≥n y validaci√≥n de datos.
- Comprender los procedimientos recomendados para la limpieza, transformaci√≥n y an√°lisis colaborativo de datos en Microsoft¬†Fabric.

## Antes de comenzar

Necesita una [Capacidad de Microsoft Fabric (F2 o superior)](https://learn.microsoft.com/fabric/fundamentals/copilot-enable-fabric) con Copilot habilitado para completar este ejercicio.

> **Nota**: Para mayor comodidad, hay disponible un cuaderno con todas las indicaciones de este ejercicio que puede descargar en:

`https://github.com/MicrosoftLearning/mslearn-fabric/raw/refs/heads/main/Allfiles/Labs/22b/Starter/eurostat-notebook.ipynb`

## Escenario del ejercicio

Imagine que Contoso Health, una red de hospitales de varias especialidades, quiere expandir sus servicios en la UE y quiere analizar los datos de poblaci√≥n proyectados. En este ejemplo se usa el conjunto de datos de proyecci√≥n de poblaci√≥n [Eurostat](https://ec.europa.eu/eurostat/web/main/home) (oficina estad√≠stica de la Uni√≥n Europea).

Origen: EUROPOP2023 Poblaci√≥n el 1 de enero por edad, sexo y tipo de proyecci√≥n [[proj_23np](https://ec.europa.eu/eurostat/databrowser/product/view/proj_23np?category=proj.proj_23n)], √öltima actualizaci√≥n el 28 de junio de 2023.

## Creaci√≥n de un √°rea de trabajo

Antes de trabajar con datos de Fabric, cree un √°rea de trabajo con Fabric habilitado. Un √°rea de trabajo de Microsoft¬†Fabric act√∫a como un entorno de colaboraci√≥n donde puede organizar y administrar todos los artefactos de ingenier√≠a de datos, incluidos almacenes de lago de datos, cuadernos y conjuntos de datos. Imagine que es como una carpeta de proyecto que contiene todos los recursos necesarios para el an√°lisis de datos.

1. En un explorador, ve a la [p√°gina principal de Microsoft Fabric](https://app.fabric.microsoft.com/home?experience=fabric) en `https://app.fabric.microsoft.com/home?experience=fabric` e inicia sesi√≥n con tus credenciales de Fabric.

1. En la barra de men√∫s de la izquierda, selecciona **√Åreas de trabajo** (el icono tiene un aspecto similar a &#128455;).

1. Cree una √°rea de trabajo con el nombre que prefiera y seleccione un modo de licencia que incluya capacidad de Fabric (*Premium* o *Fabric*). Tenga en cuenta que *Versi√≥n de evaluaci√≥n* no se admite.
   
    > **Por qu√© esto importa**: Copilot necesita una capacidad de Fabric de pago para funcionar. Esto garantiza que tiene acceso a las caracter√≠sticas con tecnolog√≠a de inteligencia artificial que ayudar√°n a generar c√≥digo a lo largo de este laboratorio.

1. Cuando se abra la nueva √°rea de trabajo, debe estar vac√≠a.

    ![Captura de pantalla de un √°rea de trabajo vac√≠a en Fabric.](./Images/new-workspace.png)

## Crear un almac√©n de lago

Ahora que tiene un √°rea de trabajo, es el momento de crear un almac√©n de lago de datos en el que ingerir los datos. Un almac√©n de lago de datos combina las ventajas de un lago de datos (almacenar datos sin procesar en varios formatos) con un almacenamiento de datos (datos estructurados optimizados para el an√°lisis). Servir√° como ubicaci√≥n de almacenamiento para los datos de poblaci√≥n sin procesar y como destino del conjunto de datos limpio y transformado.

1. En la barra de men√∫s de la izquierda, selecciona **Crear**. En la p√°gina *Nuevo*, en la secci√≥n *Ingenier√≠a de datos*, selecciona **Almac√©n de lago de datos**. As√≠gnale un nombre √∫nico que elijas.

    >**Nota**: si la opci√≥n **Crear** no est√° anclada a la barra lateral, primero debes seleccionar la opci√≥n de puntos suspensivos (**...**).

![Recorte de pantalla del bot√≥n Crear en Fabric.](./Images/copilot-fabric-notebook-create.png)

Al cabo de un minuto m√°s o menos, se crear√° un nuevo almac√©n de lago vac√≠o.

![Captura de pantalla de un nuevo almac√©n de lago.](./Images/new-lakehouse.png)

## Creaci√≥n de un cuaderno

Ahora puedes crear un cuaderno de Fabric para trabajar con los datos. Los cuadernos proporcionan un entorno interactivo donde puede escribir y ejecutar c√≥digo, visualizar los resultados y documentar el proceso de an√°lisis de datos. Son id√≥neos para el an√°lisis exploratorio de datos y el desarrollo iterativo, lo que le permite ver los resultados de cada paso inmediatamente.

1. En la barra de men√∫s de la izquierda, selecciona **Crear**. En la p√°gina *Nuevo*, en la secci√≥n *Ingenier√≠a de datos*, selecciona **Cuaderno**.

    Se crear√° y se abrir√° un nuevo cuaderno denominado **Notebook 1**.

    ![Captura de pantalla de un nuevo cuaderno.](./Images/new-notebook.png)

1. Fabric asigna un nombre a cada cuaderno que crees, como Cuaderno 1, Cuaderno 2, etc. Haz clic en el panel de nombres situado encima de la pesta√±a **Inicio** del men√∫ para cambiar el nombre a algo m√°s descriptivo.

    ![Recorte de pantalla de un nuevo cuaderno, con la capacidad de cambiar el nombre.](./Images/copilot-fabric-notebook-rename.png)

1. Selecciona la primera celda (que actualmente es una celda de c√≥digo) y, luego, en la barra de herramientas en la parte superior derecha, usa el bot√≥n **M‚Üì** para convertirla en una celda de Markdown. El texto dentro de la celda se mostrar√° como texto con formato.

    > **Por qu√© usar celdas de Markdown**: Las celdas de Markdown le permiten documentar el an√°lisis con texto con formato, lo que hace que el cuaderno sea m√°s legible y f√°cil de entender para otros usuarios (o para usted mismo cuando vuelva a √©l m√°s adelante).

    ![Recorte de pantalla de un cuaderno, en el que se cambia la primera celda para convertirla a formato Markdown.](./Images/copilot-fabric-notebook-markdown.png)

1. Usa el bot√≥n üñâ (Editar) para cambiar la celda al modo de edici√≥n y, luego, modifica el Markdown de la siguiente manera.

    ```md
    # Explore Eurostat population data.
    Use this notebook to explore population data from Eurostat
    ```
    
    ![Imagen de pantalla de un cuaderno de Fabric con una celda de markdown.](Images/copilot-fabric-notebook-step-1-created.png)
    
    Al terminar, haz clic en cualquier parte del cuaderno fuera de la celda para dejar de editarlo.

## Asociaci√≥n del almac√©n de lago de datos al cuaderno

Para trabajar con datos en el almac√©n de lago de datos desde el cuaderno, debe adjuntar el almac√©n de lago de datos al cuaderno. Esta conexi√≥n permite que el cuaderno lea y escriba en el almacenamiento del almac√©n de lago de datos, lo que crea una integraci√≥n perfecta entre el entorno de an√°lisis y el almacenamiento de datos.

1. Selecciona el √°rea de trabajo nueva en la barra izquierda. Ver√°s una lista de los elementos dentro del √°rea de trabajo, incluidos el almac√©n de lago y el cuaderno.

1. Seleccione el almac√©n de lago de datos para mostrar el panel Explorador.

1. En el men√∫ superior, selecciona **Abrir cuaderno**, **Cuaderno existente** y, a continuaci√≥n, abre el cuaderno que creaste anteriormente. El cuaderno debe estar abierto junto al panel Explorador. Expanda Almacenes de lago de datos y expanda la lista Archivos. Observe que todav√≠a no hay ninguna tabla ni archivos enumerados junto al editor de cuadernos, como se indica a continuaci√≥n:

    ![Imagen de pantalla de archivos CSV en la vista Explorador.](Images/copilot-fabric-notebook-step-2-lakehouse-attached.png)

    > **Lo que ver√°**: En el panel Explorador de la izquierda se muestra la estructura del almac√©n de lago de datos. Actualmente, est√° vac√≠o, pero a medida que se carguen y procesen datos, ver√° que los archivos aparecen en la secci√≥n **Archivos** y tablas en la secci√≥n **Tablas**.


## Cargar datos

Ahora se usar√° Copilot para ayudar a descargar datos de la API de Eurostat. En lugar de escribir c√≥digo de Python desde cero, describir√° lo que quiere hacer en lenguaje natural y Copilot generar√° el c√≥digo adecuado. Esto muestra una de las principales ventajas de la programaci√≥n asistida por IA: puede centrarse en la l√≥gica de negocios en lugar de hacerlo en los detalles de la implementaci√≥n t√©cnica.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente. Para indicar que quiere que Copilot genere c√≥digo, use `%%code` como primera instrucci√≥n de la celda. 

    > **Acerca del comando magic `%%code`**: Esta instrucci√≥n especial indica a Copilot que quiere que genere c√≥digo de Python basado en la descripci√≥n en lenguaje natural. Es uno de los muchos "comandos m√°gicos" que le ayudan a interactuar con Copilot de forma m√°s eficaz.

    ```copilot-prompt
    %%code
    
    Download the following file from this URL:
    
    https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV
     
    Then write the file to the default lakehouse into a folder named temp. Create the folder if it doesn't exist yet.
    ```
    
1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

    Copilot genera el siguiente c√≥digo SQL, que puede diferir ligeramente en funci√≥n del entorno y de las actualizaciones m√°s recientes de Copilot.
    
    ![Recorte de pantalla del c√≥digo generado por Copilot.](Images/copilot-fabric-notebook-step-3-code-magic.png)
    
    > **Funcionamiento de Copilot**: Observe c√≥mo Copilot traduce la solicitud en lenguaje natural en c√≥digo de Python funcional. Entiende que debe realizar una solicitud HTTP, controlar el sistema de archivos y guardar los datos en una ubicaci√≥n espec√≠fica del almac√©n de lago de datos.
    
    Para su comodidad, aqu√≠ est√° el c√≥digo completo por si experimentara excepciones durante la ejecuci√≥n:
    
    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import requests
    import os
    
    # Define the URL and the local path
    url = "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV"
    local_path = "/lakehouse/default/Files/temp/"
    file_name = "proj_23np.tsv"
    file_path = os.path.join(local_path, file_name)
    
    # Create the temporary directory if it doesn't exist
    if not os.path.exists(local_path):
        os.makedirs(local_path)
    
    # Download the file
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    
    # Write the content to the file
    with open(file_path, "wb") as file:
        file.write(response.content)
    
    print(f"File downloaded and saved to {file_path}")
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El archivo se debe descargar y guardar en la carpeta temporal del almac√©n de lago de datos.

    > **Nota**: Es posible que tenga que actualizar los archivos del almac√©n de lago de datos mediante la selecci√≥n de los tres puntos ...
    
    ![Recorte de pantalla de un archivo temporal creado en el almac√©n de lago de datos.](Images/copilot-fabric-notebook-step-4-lakehouse-refreshed.png)

1. Ahora que tiene el archivo de datos sin procesar en el almac√©n de lago de datos, es necesario cargarlo en un elemento DataFrame de Spark para poder analizarlo y transformarlo. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    > **Informaci√≥n**: Un elemento DataFrame es una colecci√≥n distribuida de datos organizados en columnas con nombre, similar a una tabla de una base de datos o una hoja de c√°lculo.

    ```copilot-prompt
    %%code
    
    Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    
    The fields are separated with a tab.
    
    Show the contents of the DataFrame using display method.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El elemento DataFrame datos debe contener los datos del archivo TSV. Este es un ejemplo del aspecto que podr√≠a tener el c√≥digo generado:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    # The fields have been separated with a tab.
    file_path = "Files/temp/proj_23np.tsv"
    
    spark_df = spark.read.format("csv").option("delimiter", "\t").option("header", "true").load(file_path)
    
    # Show the contents of the DataFrame using display method
    display(spark_df)
    ```

El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

| freq,projection,sex,age,unit,geo\TIME_PERIOD |      2022  |      2023  |   ...  |      2100  |
| -------------------------------------------- | ---------- | ---------- | ------ | ---------- |
|                         A,BSL,F,TOTAL,PER,AT |   4553444  |   4619179  |   ...  |   4807661  |
|                         A,BSL,F,TOTAL,PER,BE |   5883978  |   5947528  |   ...  |   6331785  |
|                         A,BSL,F,TOTAL,PER,BG |   3527626  |   3605059  |   ...  |   2543673  |
|                                          ... |       ...  |       ...  |   ...  |   5081250  |
|                         A,BSL,F,TOTAL,PER,CY |    463622  |    476907  |   ...  |    504781  |

> **Descripci√≥n de la estructura de datos**: Observe que la primera columna contiene varios valores separados por comas (frecuencia, tipo de proyecci√≥n, sexo, edad, unidad y ubicaci√≥n geogr√°fica) mientras que las columnas restantes representan a√±os con valores de poblaci√≥n. Esta estructura es com√∫n en los conjuntos de datos estad√≠sticos, pero se debe limpiar para un an√°lisis eficaz.

## Transformaci√≥n de datos: divisi√≥n de campos

Para continuar, transformar√° los datos. Es necesario asegurarse de que el primer campo se divide en columnas independientes. Adem√°s, tambi√©n es necesario asegurarse de trabajar con los tipos de datos correctos y aplicar el filtrado. 

> **Por qu√© es necesario dividir los campos**: La primera columna contiene varios fragmentos de informaci√≥n concatenados (frecuencia, tipo de proyecci√≥n, sexo, grupo de edad, unidad y c√≥digo geogr√°fico). Para un an√°lisis adecuado, cada uno debe estar en su propia columna. Este proceso se denomina "normalizaci√≥n" de la estructura de datos.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.


    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' using a comma into 6 separate fields.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import split, col
    
    # Split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' into 6 separate fields
    spark_df = spark_df.withColumn("freq", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(0)) \
                       .withColumn("projection", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(1)) \
                       .withColumn("sex", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(2)) \
                       .withColumn("age", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(3)) \
                       .withColumn("unit", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(4)) \
                       .withColumn("geo", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(5))
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo. Es posible que tenga que desplazar la tabla a la derecha para ver los nuevos campos agregados a la tabla.

    ![Recorte de pantalla de la tabla resultante con campos adicionales.](Images/copilot-fabric-notebook-split-fields.png)

## Transformaci√≥n de datos: eliminaci√≥n de campos

Algunos campos de la tabla no ofrecen ning√∫n valor significativo, ya que solo contienen una √∫nica entrada distinta. Como procedimiento recomendado, se deben quitar del conjunto de datos.

> **Principio de limpieza de datos**: Las columnas con un solo valor √∫nico no proporcionan valor anal√≠tico y pueden hacer que el conjunto de datos sea innecesariamente complejo. Al quitarlas se simplifica la estructura de datos y se mejora el rendimiento. En este caso, "freq" (frecuencia), "age" (todos los registros muestran TOTAL) y "unit" (todos los registros muestran PER para personas) son constantes en todas las filas.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, remove the fields 'freq', 'age', 'unit'.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Remove the fields 'freq', 'age', 'unit'
    spark_df = spark_df.drop("freq", "age", "unit")
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

## Transformaci√≥n de datos: reubicaci√≥n de los campos

La organizaci√≥n de los datos con las columnas de identificaci√≥n m√°s importantes primero facilita la lectura y comprensi√≥n. En el an√°lisis de datos, un procedimiento habitual consiste en colocar las columnas de categor√≠as o dimensiones (como las de tipo de proyecci√≥n, sexo y ubicaci√≥n geogr√°fica) antes que las columnas num√©ricas o de medida (los valores de poblaci√≥n por a√±o).

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, the fields 'projection', 'sex', 'geo' should be positioned first.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Reorder the DataFrame with 'projection', 'sex', 'geo' fields first
    new_column_order = ['projection', 'sex', 'geo'] + [col for col in spark_df.columns if col not in {'projection', 'sex', 'geo'}]
    spark_df = spark_df.select(new_column_order)
    
    # Show the reordered DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

## Transformaci√≥n de datos: reemplazo de valores

El campo de proyecci√≥n contiene actualmente c√≥digos cr√≠pticos que no son f√°ciles de usar. Para mejorar la legibilidad y el an√°lisis, se reemplazar√°n estos c√≥digos por nombres descriptivos que explican claramente lo que representa cada escenario de proyecci√≥n.

> **Descripci√≥n de los escenarios de proyecci√≥n**: Las organizaciones estad√≠sticas suelen usar diferentes escenarios para modelar futuros cambios de poblaci√≥n. La l√≠nea de base representa el escenario m√°s probable, mientras que las pruebas de confidencialidad muestran c√≥mo la poblaci√≥n podr√≠a cambiar en diferentes suposiciones sobre tasas de fertilidad, tasas de mortalidad y patrones de migraci√≥n.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.


    ```copilot-prompt
    %%code
    
    The 'projection' field contains codes that should be replaced with the following values:
        _'BSL' -> 'Baseline projections'.
        _'LFRT' -> 'Sensitivity test: lower fertility'.
        _'LMRT' -> 'Sensitivity test: lower mortality'.
        _'HMIGR' -> 'Sensitivity test: higher migration'.
        _'LMIGR' -> 'Sensitivity test: lower migration'.
        _'NMIGR' -> 'Sensitivity test: no migration'.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import when
    
    # Replace projection codes
    spark_df = spark_df.withColumn("projection", 
                                   when(spark_df["projection"] == "BSL", "Baseline projections")
                                   .when(spark_df["projection"] == "LFRT", "Sensitivity test: lower fertility")
                                   .when(spark_df["projection"] == "LMRT", "Sensitivity test: lower mortality")
                                   .when(spark_df["projection"] == "HMIGR", "Sensitivity test: higher migration")
                                   .when(spark_df["projection"] == "LMIGR", "Sensitivity test: lower migration")
                                   .when(spark_df["projection"] == "NMIGR", "Sensitivity test: no migration")
                                   .otherwise(spark_df["projection"]))
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

    ![Recorte de pantalla de la tabla resultante con los valores de campo de proyecci√≥n reemplazados.](Images/copilot-fabric-notebook-replace-values.png)
    
## Transformaci√≥n de datos: filtrado de datos

La tabla de proyecciones de poblaci√≥n contiene dos filas para pa√≠ses que no existen: EU27_2020 (*total para la Uni√≥n Europea: 27¬†pa√≠ses*) y EA20 (*zona Euro: 20¬†pa√≠ses*). Es necesario quitar estas dos filas, ya que quiere mantener los datos solo en el nivel m√°s bajo.

> **Principio de granularidad de los datos**: Para un an√°lisis detallado, es importante trabajar con los datos en el nivel m√°s pormenorizado posible. Los valores agregados (como los totales de la UE) siempre se pueden calcular cuando sea necesario, pero incluirlos en el conjunto de datos base puede provocar un recuento doble o confusi√≥n en el an√°lisis.

![Recorte de pantalla de la tabla con geo¬†EA20 y EU2_2020 resaltados.](Images/copilot-fabric-notebook-europe.png)

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    Filter the 'geo' field and remove values 'EA20' and 'EU27_2020' (these are not countries).
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'geo' values 'EA20' and 'EU27_2020'
    spark_df = spark_df.filter((spark_df['geo'] != 'EA20') & (spark_df['geo'] != 'EU27_2020'))
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

    La tabla de proyecci√≥n de la poblaci√≥n tambi√©n contiene un campo "sex", que contiene los siguientes valores distintos:
    
    - M: Male
    - F: Femenino
    - T: Total (hombre + mujer)

    De nuevo, es necesario quitar los totales, para mantener los datos en el nivel de detalle m√°s bajo.

    > **Por qu√© quitar totales**: Como sucede con las agregaciones geogr√°ficas, quiere mantener solo las categor√≠as de sexo individuales (Hombre y Mujer), y excluir los valores totales. Esto permite un an√°lisis m√°s flexible: siempre puede sumar los valores de Hombre y Mujer para obtener totales, pero no volver a dividir los totales en sus componentes.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    Filter the 'sex' field and remove 'T' (these are totals).
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'sex' values 'T'
    spark_df = spark_df.filter(spark_df['sex'] != 'T')
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

## Transformaci√≥n de datos: recorte de espacios

Algunos nombres de campo de la tabla de proyecci√≥n de la poblaci√≥n tienen un espacio al final. Es necesario aplicar una operaci√≥n de recorte a los nombres de estos campos.

> **El problema de la calidad de los datos**: Los espacios adicionales en los nombres de columna pueden causar problemas al consultar datos o crear visualizaciones. Se trata de un problema de calidad de datos com√∫n, especialmente cuando los datos proceden de or√≠genes externos o se exportan desde otros sistemas. El recorte de espacios garantiza la coherencia y evita problemas dif√≠ciles de depurar m√°s adelante.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    Strip spaces from all field names in the dataframe.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Strip spaces from all field names
    spark_df = spark_df.select([col(column).alias(column.strip()) for column in spark_df.columns])
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

## Transformaci√≥n de datos: conversi√≥n de tipos de datos

Si quiere analizar correctamente los datos m√°s adelante (mediante Power¬†BI o SQL, por ejemplo) debe asegurarse de que los tipos de datos (como n√∫meros y fecha y hora) est√©n configurados correctamente. 

> **Importancia de los tipos de datos correctos**: Cuando se cargan datos desde archivos de texto, inicialmente todas las columnas se tratan como cadenas. La conversi√≥n de columnas de a√±o en enteros permite operaciones matem√°ticas (como c√°lculos y agregaciones), y garantiza la ordenaci√≥n adecuada. Este paso es fundamental para las herramientas de an√°lisis y visualizaci√≥n de bajada.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    Convert the data type of all the year fields to integer.
    ```

1. Seleccione ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo y observar la salida. El ejemplo siguiente muestra el aspecto que podr√≠a tener la salida:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Convert the data type of all the year fields to integer
    year_columns = [col(column).cast("int") for column in spark_df.columns if column.strip().isdigit()]
    spark_df = spark_df.select(*spark_df.columns[:3], *year_columns)
    
    # Display the updated DataFrame
    display(spark_df)
    ```
    
1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo. Este es un ejemplo del aspecto que podr√≠a tener la salida (se han quitado las columnas y las filas para mayor brevedad):

|          proyecci√≥n|sex|geo|    2022|    2023|     ...|    2100|
|--------------------|---|---|--------|--------|--------|--------| 
|Proyecciones de l√≠nea de base|  F| AT| 4553444| 4619179|     ...| 4807661|
|Proyecciones de l√≠nea de base|  F| BE| 5883978| 5947528|     ...| 6331785|
|Proyecciones de l√≠nea de base|  F| BG| 3527626| 3605059|     ...| 2543673|
|...                 |...|...|     ...|     ...|     ...|     ...|
|Proyecciones de l√≠nea de base|  F| LU|  320333|  329401|     ...|  498954|

>[!TIP]
> Es posible que tenga que desplazar la tabla a la derecha para observar todas las columnas.

## Guardar datos

A continuaci√≥n, quiere guardar los datos transformados en el almac√©n de lago de datos. 

> **Por qu√© guardar los datos transformados**: Despu√©s de todo este trabajo de limpieza y transformaci√≥n de datos, quiere conservar los resultados. Guardar los datos como una tabla en almac√©n de lago de datos le permite usar este conjunto de datos limpio para varios escenarios de an√°lisis sin tener que repetir el proceso de transformaci√≥n. Tambi√©n permite que otras herramientas del ecosistema de Microsoft¬†Fabric (como Power¬†BI, punto de conexi√≥n de SQL Analytics y Data Factory) trabajen con estos datos.

1. Cree una celda en el cuaderno y copie dentro la instrucci√≥n siguiente.

    ```copilot-prompt
    %%code
    
    Save the dataframe as a new table named 'Population' in the default lakehouse.
    ```
    
1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo. Copilot genera c√≥digo, que puede diferir ligeramente en funci√≥n del entorno y de las actualizaciones m√°s recientes de Copilot.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    spark_df.write.format("delta").saveAsTable("Population")
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

## Validaci√≥n: formulaci√≥n de preguntas

Ahora, explorar√° la eficacia de Copilot para el an√°lisis de datos. En lugar de escribir consultas SQL complejas o c√≥digo de visualizaci√≥n desde cero, puede formular a Copilot preguntas en lenguaje natural sobre los datos y generar√° el c√≥digo adecuado para responderlas.

1. Para validar que los datos se han guardado correctamente, expanda las tablas del almac√©n de lago de datos y compruebe el contenido (es posible que tenga que seleccionar los tres puntos ... para actualizar la carpeta Tables). 

    ![Recorte de pantalla del almac√©n de lago de datos que ahora contiene una nueva tabla denominada "Population".](Images/copilot-fabric-notebook-step-5-lakehouse-refreshed.png)

1. En la cinta Inicio, seleccione la opci√≥n Copilot.

    > **Interfaz de chat de Copilot**: En el panel de Copilot se proporciona una interfaz de conversaci√≥n en la que puede formular preguntas sobre los datos en lenguaje natural. Puede generar c√≥digo para el an√°lisis, crear visualizaciones y ayudarle a explorar patrones en el conjunto de datos.

    ![Recorte de pantalla del cuaderno con el panel Copilot abierto.](Images/copilot-fabric-notebook-step-6-copilot-pane.png)

1. Escriba lo siguiente:

    ```copilot-prompt
    What are the projected population trends for geo BE  from 2020 to 2050 as a line chart visualization. Make sure to sum up male and female numbers. Use only existing columns from the population table. Perform the query using SQL.
    ```

    > **Qu√© demuestra esto**: En este mensaje se muestra la capacidad de Copilot para comprender el contexto (la tabla Population), generar consultas SQL y crear visualizaciones. Es especialmente eficaz porque combina consultas de datos con visualizaci√≥n en una sola solicitud.

1. Observe la salida generada, que puede diferir ligeramente en funci√≥n del entorno y de las actualizaciones m√°s recientes de Copilot. Copie el fragmento de c√≥digo en una nueva celda.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import plotly.graph_objs as go
    
    # Perform the SQL query to get projected population trends for geo BE, summing up male and female numbers
    result = spark.sql(
        """
        SELECT projection, sex, geo, SUM(`2022`) as `2022`, SUM(`2023`) as `2023`, SUM(`2025`) as `2025`,
               SUM(`2030`) as `2030`, SUM(`2035`) as `2035`, SUM(`2040`) as `2040`,
               SUM(`2045`) as `2045`, SUM(`2050`) as `2050`
        FROM Population
        WHERE geo = 'BE' AND projection = 'Baseline projections'
        GROUP BY projection, sex, geo
        """
    )
    df = result.groupBy("projection").sum()
    df = df.orderBy("projection").toPandas()
    
    # Extract data for the line chart
    years = df.columns[1:].tolist()
    values = df.iloc[0, 1:].tolist()
    
    # Create the plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=years, y=values, mode='lines+markers', name='Projected Population'))
    
    # Update the layout
    fig.update_layout(
        title='Projected Population Trends for Geo BE (Belgium) from 2022 to 2050',
        xaxis_title='Year',
        yaxis_title='Population',
        template='plotly_dark'
    )
    
    # Display the plot
    fig.show()
    ```

1. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo. 

    Observe el gr√°fico que ha creado:
    
    ![Recorte de pantalla del cuaderno con el gr√°fico de l√≠neas creado.](Images/copilot-fabric-notebook-step-7-line-chart.png)
    
    > **Qu√© ha conseguido**: Ha usado correctamente Copilot para generar una visualizaci√≥n que muestra las tendencias de poblaci√≥n de B√©lgica en el tiempo. Esto muestra el flujo de trabajo de ingenier√≠a de datos de un extremo a otro: ingesta de datos, transformaci√≥n, almacenamiento y an√°lisis, todo ello con ayuda de la inteligencia artificial.

## Limpieza de recursos

En este ejercicio, ha aprendido a usar Copilot y Spark para trabajar con datos en Microsoft¬†Fabric.

Si has terminado de explorar los datos, puedes terminar la sesi√≥n de Spark y eliminar el √°rea de trabajo que has creado para este ejercicio.

1.  En el men√∫ del cuaderno, selecciona **Detener sesi√≥n** para finalizar la sesi√≥n con Spark.
1.  En la barra de la izquierda, seleccione el icono del √°rea de trabajo para ver todos los elementos que contiene.
1.  Selecciona **Configuraci√≥n del √°rea de trabajo** y, en la secci√≥n **General**, despl√°zate hacia abajo y selecciona **Quitar esta √°rea de trabajo**.
1.  Selecciona **Eliminar** para eliminar el √°rea de trabajo.
