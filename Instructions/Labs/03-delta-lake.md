---
lab:
  title: Uso de tablas Delta en Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Uso de tablas Delta en Apache Spark

Las tablas de un almac√©n de lago de Microsoft¬†Fabric se basan en el formato Delta Lake de c√≥digo abierto. Delta Lake agrega compatibilidad con la sem√°ntica relacional para los datos por lotes y de streaming. En este ejercicio, crear√°s tablas Delta y explorar√°s los datos mediante consultas SQL.

Este ejercicio deber√≠a tardar en completarse 45 minutos aproximadamente

> [!NOTE]
> Necesitar√°s una versi√≥n de prueba de [Microsoft¬†Fabric](/fabric/get-started/fabric-trial) para realizar este ejercicio.

## Creaci√≥n de un √°rea de trabajo

Primero, crea un √°rea de trabajo con la *versi√≥n de prueba* de Fabric habilitada.

1. En la p√°gina principal de Microsoft Fabric en https://app.fabric.microsoft.com, selecciona la experiencia **Synapse: ingenier√≠a de datos**.
1. En la barra de men√∫s de la izquierda, selecciona el **√Åreas de trabajo** (üóá).
1. Crea una **nueva √°rea de trabajo** con el nombre que prefieras y selecciona un modo de licencia que incluya capacidad de Fabric (versi√≥n de prueba, Premium o Fabric).
1. Cuando se abra la nueva √°rea de trabajo, debe estar vac√≠a.

![Imagen de pantalla de un √°rea de trabajo de Fabric vac√≠a.](Images/workspace-empty.jpg)

## Creaci√≥n de un almac√©n de lago y carga de datos

Ahora que tienes un √°rea de trabajo, es el momento de crear un almac√©n de lago de datos y cargar algunos archivos.

1. En la p√°gina principal de **Synapse :Ingenier√≠a de datos**, crea un nuevo **almac√©n de lago** con el nombre que prefieras. 
1. Hay varias maneras de ingerir datos, pero en este ejercicio descargar√°s un archivo de texto en el equipo local (o m√°quina virtual de laboratorio, si procede) y, luego, lo cargar√°s en el almac√©n de lago. Descarga el [archivo de datos](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) de https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv, gu√°rdalo como *products.csv*.
1.  Vuelve a la pesta√±a del explorador web que contiene el almac√©n de lago y, en el panel Explorador, junto a la carpeta **Archivos**, selecciona ... men√∫.  Crea una **Nueva subcarpeta** denominada *products*.
1.  En ... men√∫ de la carpeta products, **carga** el archivo *products.csv* desde el equipo local (o m√°quina virtual de laboratorio, si procede).
1.  Una vez cargado el archivo, selecciona la carpeta **products** y comprueba que se ha cargado el archivo, como se muestra aqu√≠:

![Imagen de pantalla de products.csv cargado en el almac√©n de lago.](Images/upload-products.jpg)
  
## Exploraci√≥n de datos en DataFrame

1.  Crea un **nuevo cuaderno**. Al cabo de unos segundos, se abrir√° un nuevo cuaderno que contiene una sola celda. Los cuadernos se componen de una o varias celdas que pueden contener c√≥digo o Markdown (texto con formato).
2.  Selecciona la primera celda (que actualmente es una celda de c√≥digo) y, luego, en la barra de herramientas en la parte superior derecha, usa el bot√≥n **M‚Üì** para convertirla en una celda de Markdown. El texto dentro de la celda se mostrar√° como texto con formato. Usa las celdas de Markdown para proporcionar informaci√≥n explicativa sobre el c√≥digo.
3.  Usa el bot√≥n üñâ (Editar) para cambiar la celda al modo de edici√≥n y, luego, modifica el Markdown de la siguiente manera:

```markdown
# Delta Lake tables 
Use this notebook to explore Delta Lake functionality 
```

4. Haz clic en cualquier parte del cuaderno fuera de la celda para dejar de editarlo y ver el Markdown representado.
5. Agrega una nueva celda de c√≥digo y agrega el c√≥digo siguiente para leer los datos de productos en un DataFrame mediante un esquema definido:

```python
from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

# define the schema
schema = StructType() \
.add("ProductID", IntegerType(), True) \
.add("ProductName", StringType(), True) \
.add("Category", StringType(), True) \
.add("ListPrice", DoubleType(), True)

df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
# df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
display(df)
```

> [!TIP]
> Oculta o muestra los paneles del explorador mediante el icono del bot√≥n de contenido adicional ¬´. Esto te permite centrarte en el cuaderno o en los archivos.

7. Usa el bot√≥n **Ejecutar celda** (‚ñ∑) situado a la izquierda de la celda para ejecutarla.

> [!NOTE]
> Dado que esta es la primera vez que ejecutas c√≥digo en este cuaderno, debes iniciar una sesi√≥n con Spark. Esto significa que la primera ejecuci√≥n puede tardar un minuto en completarse. Las ejecuciones posteriores ser√°n m√°s r√°pidas.

8. Cuando se haya completado el c√≥digo de celda, revisa la salida que aparece debajo de ella, que ser√° algo parecido a esto:

![Imagen de pantalla de los datos de products.csv.](Images/products-schema.jpg)
 
## Creaci√≥n de tablas Delta

Puedes guardar el DataFrame como una tabla Delta mediante el m√©todo *saveAsTable*. Delta Lake admite la creaci√≥n de tablas administradas y externas:

* Las tablas Delta **administradas** se benefician del mayor rendimiento, ya que Fabric administra los metadatos del esquema y los archivos de datos.
* Las tablas **externas** permiten almacenar datos externamente, con los metadatos administrados por Fabric.

### Creaci√≥n de una tabla administrada

Los archivos de datos se crean en la carpeta **Tables**.

1. En los resultados devueltos por la primera celda de c√≥digo, usa el icono + C√≥digo para agregar una nueva celda de c√≥digo.

> [!TIP]
> Para ver el icono + C√≥digo, mueve el rat√≥n hasta justo debajo y a la izquierda de la salida de la celda actual. Como alternativa, en la barra de men√∫s, en la pesta√±a Editar, selecciona **+ Agregar celda de c√≥digo**.

2. Crea una tabla Delta administrada, agrega una nueva celda, escribe el siguiente c√≥digo y, despu√©s, ejec√∫tala:

```python
df.write.format("delta").saveAsTable("managed_products")
```

3.  En el panel del explorador del almac√©n de lago, **actualiza** la carpeta Tables y ampl√≠a el nodo Tables para comprobar que se ha creado la tabla **managed_products**.

>[!NOTE]
> El icono de tri√°ngulo situado junto al nombre de archivo indica una tabla Delta.

Los archivos de la tabla administrada se almacenan en la carpeta **Tables** del almac√©n de lago. Se ha creado una carpeta llamada *managed_products* para almacenar los archivos Parquet y la carpeta delta_log para la tabla que has creado.

### Creaci√≥n de una tabla externa

Tambi√©n puedes crear tablas externas, que se pueden almacenar en alg√∫n lugar distinto del almac√©n de lago, con los metadatos de esquema almacenados en el almac√©n de lago.

1.  En el panel Explorador del almac√©n de lago, en ... men√∫ de la carpeta **Archivos**, selecciona **Copiar ruta de acceso de ABFS**. La ruta de acceso de ABFS es la ruta de acceso completa a la carpeta Archivos del almac√©n de lago.

2.  En una nueva celda de c√≥digo, pega la ruta de acceso de ABFS. Agrega el c√≥digo siguiente mediante cortar y pegar para insertar abfs_path en el lugar correcto en el c√≥digo:

```python
df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
```

La ruta de acceso completa debe ser similar a la siguiente:

```python
abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
```

4. **Ejecuta** la celda para guardar el DataFrame como una tabla externa en la carpeta Files/external_products.

5.  En el panel Explorador del almac√©n de lago, **actualiza** la carpeta Tables y ampl√≠a el nodo Tables. Comprueba que se ha creado la tabla external_products que contiene los metadatos del esquema.

6.  En el panel Explorador del almac√©n de lago, en ... men√∫ de la carpeta Archivos, selecciona **Actualizar**. A continuaci√≥n, expande el nodo Archivos y comprueba que se ha creado la carpeta external_products para los archivos de datos de la tabla.

### Comparaci√≥n de tablas administradas y externas

Vamos a explorar las diferencias entre tablas administradas y externas mediante el comando m√°gico %%sql.

1. En la nueva celda de c√≥digo, ejecuta el c√≥digo siguiente:

```pthon
%%sql
DESCRIBE FORMATTED managed_products;
```

2. En los resultados, mira la propiedad Location de la tabla. Haz clic en el valor Location de la columna Tipo de datos para ver la ruta de acceso completa. Observa que la ubicaci√≥n de almacenamiento de OneLake termina con /Tables/managed_products.

3. Modifica el comando DESCRIBE para mostrar los detalles de la tabla external_products como se muestra aqu√≠:

```python
%%sql
DESCRIBE FORMATTED external_products;
```

4. Ejecuta la celda y, en los resultados, mira la propiedad Location de la tabla. Ampl√≠a la columna Tipo de datos para ver la ruta de acceso completa y observa que las ubicaciones de almacenamiento de OneLake terminan con /Files/external_products.

5. En la nueva celda de c√≥digo, ejecuta el c√≥digo siguiente:

```python
%%sql
DROP TABLE managed_products;
DROP TABLE external_products;
```

6. En el panel explorador del almac√©n de lago, **actualiza** la carpeta Tables para comprobar que no se muestran tablas en el nodo Tables.
7.  En el panel Explorador del almac√©n de lago, **actualiza** la carpeta Archivos y comprueba que la tabla external_products *no* se ha eliminado. Selecciona esta carpeta para ver los archivos de datos de Parquet y la carpeta _delta_log. 

Los metadatos de la tabla externa se eliminaron, pero no el archivo de datos.

## Uso de SQL para crear una tabla Delta

Ahora crear√°s una tabla Delta con el comando magic %%sql. 

1. Agrega otra celda de c√≥digo y escribe el c√≥digo siguiente:

```python
%%sql
CREATE TABLE products
USING DELTA
LOCATION 'Files/external_products';
```

2. En el panel Explorador del almac√©n de lago, en ... men√∫ de la carpeta **Tables** selecciona **Actualizar**. A continuaci√≥n, expanda el nodo Tables y comprueba que aparece una nueva tabla llamada *products*. A continuaci√≥n, expande la tabla para ver el esquema.

3. Agrega otra celda de c√≥digo y escribe el c√≥digo siguiente:

```python
%%sql
SELECT * FROM products;
```

## Exploraci√≥n del control de versiones de tabla

El historial de transacciones de las tablas Delta se almacena en archivos JSON en la carpeta delta_log. Puedes usar este registro de transacciones para administrar el control de versiones de los datos.

1.  Agrega una nueva celda de c√≥digo al cuaderno y ejecuta el c√≥digo siguiente que implementa una reducci√≥n del 10 % en el precio de las bicicletas de monta√±a:

```python
%%sql
UPDATE products
SET ListPrice = ListPrice * 0.9
WHERE Category = 'Mountain Bikes';
```

2. Agrega otra celda de c√≥digo y escribe el c√≥digo siguiente:

```python
%%sql
DESCRIBE HISTORY products;
```

Los resultados muestran el historial de transacciones registradas para la tabla.

3.  Agrega otra celda y escribe el c√≥digo siguiente:

```python
delta_table_path = 'Files/external_products'
# Get the current data
current_data = spark.read.format("delta").load(delta_table_path)
display(current_data)

# Get the version 0 data
original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
display(original_data)
```

Se devuelven dos conjuntos de resultados: uno que contiene los datos despu√©s de la reducci√≥n de precios y el otro que muestra la versi√≥n original de los datos.

## An√°lisis de datos de tabla Delta con consultas SQL

Con el comando magic de SQL, puedes usar la sintaxis SQL en lugar de Pyspark. Aqu√≠ crear√°s una vista temporal a partir de la tabla products mediante una instrucci√≥n `SELECT`.

1. Agrega una nueva celda de c√≥digo y ejecuta el c√≥digo siguiente para crear y mostrar la vista temporal:

```python
%%sql
-- Create a temporary view
CREATE OR REPLACE TEMPORARY VIEW products_view
AS
    SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
        FROM products
        GROUP BY Category;

SELECT *
    FROM products_view
    ORDER BY Category;
        
```

2. Agrega una nueva celda de c√≥digo y ejecuta el c√≥digo siguiente para devolver las 10 categor√≠as principales por n√∫mero de productos:

```python
%%sql
SELECT Category, NumProducts
    FROM products_view
    ORDER BY NumProducts DESC
    LIMIT 10;
```

Cuando se devuelvan los datos, selecciona la vista **Gr√°fico** para mostrar un gr√°fico de barras.

![Imagen de pantalla de la instrucci√≥n select y los resultados de SQL.](Images/sql-select.jpg)

Como alternativa, puedes ejecutar una consulta SQL mediante PySpark.

1. Agrega una nueva celda de c√≥digo y ejecuta el c√≥digo siguiente:

```python
from pyspark.sql.functions import col, desc

df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
display(df_products.limit(6))
```

## Uso de tablas Delta para datos de streaming

Delta Lake admite datos de streaming. Las tablas Delta pueden ser un receptor o un origen para flujos de datos creados mediante Spark Structured Streaming API. En este ejemplo, usar√°s una tabla Delta como receptor para algunos datos de streaming en un escenario simulado de Internet de las cosas (IoT).

1.  Agrega una nueva celda de c√≥digo con el c√≥digo siguiente y ejec√∫talo:

```python
from notebookutils import mssparkutils
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Create a folder
inputPath = 'Files/data/'
mssparkutils.fs.mkdirs(inputPath)

# Create a stream that reads data from the folder, using a JSON schema
jsonSchema = StructType([
StructField("device", StringType(), False),
StructField("status", StringType(), False)
])
iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

# Write some event data to the folder
device_data = '''{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"ok"}
{"device":"Dev2","status":"error"}
{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"error"}
{"device":"Dev2","status":"ok"}
{"device":"Dev2","status":"error"}
{"device":"Dev1","status":"ok"}'''

mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

print("Source stream created...")
```

Aseg√∫rate de que se muestra el mensaje *Flujo de origen creado...* . El c√≥digo que acabas de ejecutar ha creado un origen de datos de streaming basado en una carpeta en la que se han guardado algunos datos, que representan lecturas de dispositivos IoT hipot√©ticos.

2. En la nueva celda de c√≥digo, agrega el c√≥digo siguiente:

```python
# Write the stream to a delta table
delta_stream_table_path = 'Tables/iotdevicedata'
checkpointpath = 'Files/delta/checkpoint'
deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
print("Streaming to delta sink...")
```

Este c√≥digo escribe los datos del dispositivo de streaming en formato Delta en una carpeta llamada iotdevicedata. Como la ruta de acceso de la ubicaci√≥n de carpeta es la carpeta Tablas, se crear√° autom√°ticamente una tabla para ella.

3. En la nueva celda de c√≥digo, agrega el c√≥digo siguiente:

```python
%%sql
SELECT * FROM IotDeviceData;
```

Este c√≥digo consulta la tabla IotDeviceData, que contiene los datos del dispositivo del origen de streaming.

4. En la nueva celda de c√≥digo, agrega el c√≥digo siguiente:

```python
# Add more data to the source stream
more_data = '''{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"ok"}
{"device":"Dev1","status":"error"}
{"device":"Dev2","status":"error"}
{"device":"Dev1","status":"ok"}'''

mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
```

Este c√≥digo escribe datos de dispositivo m√°s hipot√©ticos en el origen de streaming.

5. Vuelve a ejecutar la celda que contiene el c√≥digo siguiente:

```python
%%sql
SELECT * FROM IotDeviceData;
```

Este c√≥digo consulta de nuevo la tabla IotDeviceData, que ahora debe incluir los datos adicionales que se agregaron al origen de streaming.

6. En una nueva celda de c√≥digo, agrega c√≥digo para detener la secuencia y ejecutar la celda:

```python
deltastream.stop()
```

## Limpieza de recursos

En este ejercicio, has aprendido a trabajar con tablas Delta en Microsoft Fabric.

Si has terminado de explorar el almac√©n de lago, puedes eliminar el √°rea de trabajo que has creado para este ejercicio.

1. En la barra de la izquierda, selecciona el icono del √°rea de trabajo para ver todos los elementos que contiene.
2. En ... men√∫ de la barra de herramientas, selecciona **Configuraci√≥n del √°rea de trabajo**.
3. En la secci√≥n General, selecciona **Quitar esta √°rea de trabajo**.
