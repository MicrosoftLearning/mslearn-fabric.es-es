---
lab:
  title: An√°lisis de datos con Apache Spark
  module: Use Apache Spark to work with files in a lakehouse
---

# An√°lisis de datos con Apache Spark en Fabric

En este laboratorio, ingerir√°s datos en el almac√©n de lago de Fabric y usar√°s PySpark para leer y analizarlos.

Este laboratorio tardar√° aproximadamente 45¬†minutos en completarse.

## Requisitos previos

* Una [versi√≥n de prueba de Microsoft Fabric](/fabric/get-started/fabric-trial#start-the-fabric-capacity-trial)

## Creaci√≥n de un √°rea de trabajo

Para poder trabajar con datos en Fabric, debes crear un √°rea de trabajo.

1. En la p√°gina principal de [Microsoft Fabric](https://app.fabric.microsoft.com) en https://app.fabric.microsoft.com, selecciona experiencia de **Ingenier√≠a de datos**.
1. En la barra de men√∫ izquierda, selecciona **√Åreas de trabajo** (üóá) y **Nueva √°rea de trabajo**.
1. Asigna un nombre a la nueva √°rea de trabajo y, en la secci√≥n **Avanzado**, selecciona el modo de licencia adecuado. Si has iniciado una versi√≥n de prueba de Microsoft Fabric, selecciona Prueba.
1. Selecciona **Aplicar** para crear un √°rea de trabajo vac√≠a.
 
![Imagen de pantalla de los archivos CSV cargados en una nueva √°rea de trabajo de Fabric.](Images/uploaded-files.jpg)

## Creaci√≥n de un almac√©n de lago y carga de archivos

Ahora que tienes un √°rea de trabajo, puedes crear un almac√©n de lago para los archivos de datos. En el √°rea de trabajo nueva, selecciona **Nuevo** y **Almac√©n de lago**. Asigna un nombre al almac√©n de lago y selecciona **Crear**. Despu√©s de un breve retraso, se crea un nuevo almac√©n de lago.

Ahora puedes ingerir datos en el almac√©n de lago. Hay varias maneras de hacerlo, pero de momento descargar√°s y extraer√°s una carpeta de archivos de texto en el equipo local (o m√°quina virtual de laboratorio si procede) y, luego, los cargar√°s en el almac√©n de lago.

1. Descarga los archivos de datos de https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip.
1. Extraer el archivo comprimido y comprueba que tienes una carpeta denominada *orders* que contiene los archivos CSV 2019.csv, 2020.csvy 2021.csv.
1. Vuelve a tu nuevo almac√©n de lago. En el panel **Explorador**, selecciona el men√∫ de puntos suspensivos **...** situado junto a la carpeta **Archivos** despu√©s, selecciona **Cargar** y **Cargar carpeta**. Ve a la carpeta orders del equipo local (o m√°quina virtual de laboratorio si procede) y selecciona **Cargar**.
1. Una vez cargados los archivos, expande **Archivos** y selecciona la carpeta **orders**. Comprueba que se ha cargado el archivo CSV, como se muestra aqu√≠:

![Imagen de pantalla de una nuevo √°rea de trabajo de Fabric.](Images/new-workspace.jpg)

## Creaci√≥n de un cuaderno

Ahora puedes crear un cuaderno de Fabric para trabajar con los datos. Los cuadernos proporcionan un entorno interactivo en el que puedes escribir y ejecutar c√≥digo.

1. Selecciona el √°rea de trabajo y, a continuaci√≥n, selecciona **Nuevo** y **Cuaderno**. Al cabo de unos segundos, se abrir√° un nuevo cuaderno que contiene una sola celda. Los cuadernos se componen de una o varias celdas que pueden contener c√≥digo o Markdown (texto con formato).
1. Fabric asigna un nombre a cada cuaderno que crees, como Cuaderno 1, Cuaderno 2, etc. Haz clic en el panel de nombres situado encima de la pesta√±a **Inicio** del men√∫ para cambiar el nombre a algo m√°s descriptivo.
1. Selecciona la primera celda (que actualmente es una celda de c√≥digo) y, luego, en la barra de herramientas en la parte superior derecha, usa el bot√≥n **M‚Üì** para convertirla en una celda de Markdown. El texto dentro de la celda se mostrar√° como texto con formato.
1. Usa el bot√≥n üñâ (Editar) para cambiar la celda al modo de edici√≥n y, luego, modifica el Markdown de la siguiente manera.

```markdown
# Sales order data exploration
Use this notebook to explore sales order data
```
![Imagen de pantalla de un cuaderno de Fabric con una celda de markdown.](Images/name-notebook-markdown.jpg)

Al terminar, haz clic en cualquier parte del cuaderno fuera de la celda para dejar de editarlo y ver el Markdown representado.


## Creaci√≥n de un DataFrame

Ahora que has creado una √°rea de trabajo, un almac√©n de lago y un cuaderno est√°s listo para trabajar con los datos. Usar√°s PySpark, que es el lenguaje predeterminado para cuadernos de Fabric y la versi√≥n de Python optimizada para Spark.

**NOTA:** los cuadernos de Fabric admiten varios lenguajes de programaci√≥n, como Scala, R y Spark SQL.

1. Selecciona el √°rea de trabajo nueva en la barra izquierda. Ver√°s una lista de los elementos dentro del √°rea de trabajo, incluidos el almac√©n de lago y el cuaderno.
2. Selecciona el almac√©n de lago para mostrar el panel Explorador, incluida la carpeta **orders**.
3. En el men√∫ superior, selecciona **Abrir cuaderno**, **Bloc de notas existente** y, a continuaci√≥n, abre el cuaderno que creaste anteriormente. El cuaderno debe estar abierto junto al panel Explorador. Expande los almacenes de lago, expande la lista Archivos y selecciona la carpeta orders. Los archivos CSV que cargaste se muestran junto al editor de cuadernos, de la siguiente manera:

![Imagen de pantalla de archivos CSV en la vista Explorador.](Images/explorer-notebook-view.jpg)

4. En ... el men√∫ de 2019.csv, selecciona **Cargar datos** > **Spark**. El c√≥digo siguiente se generar√° autom√°ticamente en una nueva celda de c√≥digo:

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

**Sugerencia:** puedes ocultar los paneles del explorador del almac√©n de lago de la izquierda mediante los iconos ¬´. Esto proporciona m√°s espacio para el cuaderno.

5. Selecciona ‚ñ∑ **Ejecutar celda** a la izquierda de la celda para ejecutar el c√≥digo.

**NOTA**: la primera vez que ejecutas c√≥digo de Spark, se inicia una sesi√≥n de Spark. Esto puede tardar unos segundos o m√°s. Las ejecuciones posteriores dentro de la misma sesi√≥n ser√°n m√°s r√°pidas.

6. Cuando se haya completado el c√≥digo de celda, revisa la salida que aparece debajo de ella, que ser√° algo parecido a esto:
 
![Imagen de pantalla que muestra el c√≥digo y los datos generados autom√°ticamente.](Images/auto-generated-load.jpg)

7. La salida muestra las filas y columnas de datos del archivo 2019.csv.  Observa que los encabezados de columna contienen la primera l√≠nea de los datos. Para corregir esto, debes modificar la primera l√≠nea del c√≥digo de la siguiente manera:

```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

8. Vuelve a ejecutar el c√≥digo para que DataFrame identifique correctamente la primera fila como datos. Observa que los nombres de columna han cambiado ahora a _c0, _c1, etc.

9. Los nombres de columna descriptivos te ayudan a comprender los datos. Para crear nombres de columna significativos, debes definir el esquema y los tipos de datos. Tambi√©n debes importar un conjunto est√°ndar de tipos de Spark SQL para definir los tipos de datos. Reemplaza el c√≥digo existente por el siguiente:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)

```
10. Ejecuta la celda y revisa la salida:

![Imagen de pantalla del c√≥digo con datos y definidos por el esquema.](Images/define-schema.jpg)

11. Este DataFrame solo incluye los datos del archivo 2019.csv. Modifica el c√≥digo para que la ruta de acceso del archivo use un car√°cter comod√≠n* para leer todos los archivos de la carpeta orders:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

12. Al ejecutar el c√≥digo modificado, deber√≠as ver las ventas de 2019, 2020 y 2021. Solo se muestra un subconjunto de las filas, por lo que es posible que no veas las filas de cada a√±o.

**NOTA:** puedes ocultar o mostrar la salida de una celda seleccionando ... junto al resultado. Esto facilita el trabajo en un cuaderno.

## Exploraci√≥n de datos en DataFrame

El objeto DataFrame proporciona funcionalidad adicional, como la capacidad de filtrar, agrupar y manipular datos.

### Filtrado de un DataFrame

1. Agrega una celda de c√≥digo seleccionando **+ C√≥digo** que aparece al mantener el puntero encima o debajo de la celda actual o su salida. Como alternativa, en el men√∫ de cinta, selecciona **Editar** y **+ Agregar** celda de c√≥digo.

2.  El c√≥digo siguiente filtra los datos para que solo se devuelvan dos columnas. Tambi√©n usa *count* y *distinct* para resumir el n√∫mero de registros:

```python
customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

3. Ejecuta el c√≥digo y examina la salida:

* El c√≥digo crea un nuevo DataFrame denominado **customers** que contiene un subconjunto de columnas del DataFrame **df** original. Al realizar una transformaci√≥n DataFrame no se modifica el DataFrame original, pero se devuelve uno nuevo.
* Otra manera de lograr el mismo resultado es usar el m√©todo select:

```
customers = df.select("CustomerName", "Email")
```

* Las funciones de DataFrame *count* y *distinct* se usan para proporcionar totales para el n√∫mero de clientes y clientes √∫nicos.

4. Modifica la primera l√≠nea del c√≥digo mediante *select* con una funci√≥n *where* como se indica a continuaci√≥n:

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

5. Ejecuta el c√≥digo modificado para seleccionar los clientes que han comprado el producto Road-250 Red, 52. Ten en cuenta que puedes "encadenar" varias funciones juntas para que la salida de una funci√≥n se convierta en la entrada para la siguiente. En este caso, el DataFrame creado por el m√©todo *select* es el DataFrame de origen del m√©todo **where** se usa para aplicar criterios de filtrado.

### Agregaci√≥n y agrupaci√≥n de datos en un DataFrame

1. Agregue una celda de c√≥digo y escribe el c√≥digo siguiente:

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

2. Ejecuta el c√≥digo. Observa que los resultados muestran la suma de las cantidades de pedidos agrupadas por producto. El m√©todo *groupBy* agrupa las filas por elemento y la funci√≥n de agregado *sum* subsiguiente se aplica a todas las columnas num√©ricas restantes (en este caso, *Quantity*).

3. Agrega otra nueva celda de c√≥digo al cuaderno y escribe en ella el c√≥digo siguiente:

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

4. Ejecuta la celda. Examina los resultados. Los resultados muestran el n√∫mero de pedidos de ventas por a√±o:

* La instrucci√≥n *import* permite usar la biblioteca de Spark SQL.
* El m√©todo *select* se usa con una funci√≥n SQL year para extraer el componente year del campo *OrderDate*.
* Se usa el m√©todo *alias* para asignar un nombre de columna al valor de a√±o extra√≠do.
* El m√©todo *groupBy* agrupa los datos por la columna Year derivada.
* El recuento de filas de cada grupo se calcula antes de que se use el m√©todo *orderBy* para ordenar el DataFrame resultante.

![Imagen de pantalla que muestra los resultados de la agregaci√≥n y agrupaci√≥n de datos en un DataFrame.](Images/spark-sql-dataframe.jpg)

## Uso de Spark para transformar archivos de datos

Una tarea com√∫n para los ingenieros y cient√≠ficos de datos es transformar datos para su posterior procesamiento o an√°lisis.

### Uso de m√©todos y funciones de DataFrame para transformar datos

1. Agrega una celda de c√≥digo al cuaderno y escribe el c√≥digo siguiente:

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```

2. Ejecuta la celda. Se crear un nuevo DataFrame a partir de los datos de pedido originales con las siguientes transformaciones:

- Agrega las columnas Year y Month basadas en la columna OrderDate.
- Agrega las columnas FirstName y LastName basadas en la columna CustomerName.
- Filtra y reordena las columnas, quitando la columna CustomerName.

3. Revisa la salida y comprueba que las transformaciones se han realizado en los datos.

Puedes usar la biblioteca de Spark SQL para transformar los datos filtrando filas, derivando, quitando y cambiando el nombre de columnas y aplicando cualquier otra modificaci√≥n de datos.

>[!TIP]
> Consulta la documentaci√≥n de [DataFrame de Apache Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) para m√°s informaci√≥n sobre DataFrame.

### Guardado de los datos transformados

En este momento, es posible que quieras guardar los datos transformados para que se puedan usar para realizar un an√°lisis posterior.

*Parquet* es un formato de almacenamiento de datos popular porque almacena los datos de forma eficaz y es compatible con la mayor√≠a de los sistemas de an√°lisis de datos a gran escala. De hecho, a veces el requisito de transformaci√≥n de datos puede ser convertir datos de otro formato, como CSV, a Parquet.

1. Para guardar el DataFrame transformado en formato Parquet, agrega una celda de c√≥digo y agrega el c√≥digo siguiente:  

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```

2. Ejecuta la celda y espera a que aparezca el mensaje de que se han guardado los datos. A continuaci√≥n, en el panel Almac√©n de lago de la izquierda, en ... el men√∫ del nodo Archivos, selecciona **Actualizar**. Selecciona la carpeta transformed_data para comprobar que contiene una nueva carpeta denominada orders, que a su vez contiene uno o varios archivos Parquet.

3. Agrega una celda con el c√≥digo siguiente:

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

4. Ejecuta la celda.  Se crea un nuevo DataFrame a partir de los archivos Parquet de la carpeta *transformed_data/orders*. Comprueba que los resultados muestran los datos de pedido que se han cargado desde los archivos Parquet.

![Imagen de pantalla que muestra archivos Parquet.](Images/parquet-files.jpg)

### Guardado de datos en archivos con particiones

Al tratar con grandes vol√∫menes de datos, la creaci√≥n de particiones puede mejorar significativamente el rendimiento y facilitar el filtrado de datos.

1. Agrega una nueva celda con c√≥digo para guardar el DataFrame, y particiona los datos por Year y Month:

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

2.  Ejecuta la celda y espera a que aparezca el mensaje de que se han guardado los datos. A continuaci√≥n, en el panel Almac√©n de lago de la izquierda, en ... el men√∫ ... del nodo Archivos, selecciona **Actualizar** y expande la carpeta partitioned_orders para comprobar que contiene una jerarqu√≠a de carpetas llamadas *Year=xxxx*, cada una de las cuales contiene carpetas llamadas *Month=xxxx*. Cada carpeta Month contiene un archivo Parquet con los pedidos de ese mes.

![Imagen de pantalla en la que se muestran los datos con particiones por a√±o y mes.](Images/partitioned-data.jpg)

3. Agrega una nueva celda con el c√≥digo siguiente para cargar un nuevo DataFrame desde el archivo orders.parquet:

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```

4. Ejecuta la celda y comprueba que los resultados muestran los datos de pedido de ventas de 2021. Ten en cuenta que las columnas con particiones especificadas en la ruta de acceso (Year y Month) no se incluyen en el DataFrame.

## Trabajo con tablas y SQL

Como se ha visto, los m√©todos nativos del objeto DataFrame te permiten consultar y analizar datos de un archivo. Sin embargo, es posible que te resulte m√°s c√≥modo trabajar con tablas mediante la sintaxis SQL. Spark proporciona un metastore en el que puedes definir tablas relacionales. 

La biblioteca de Spark SQL admite el uso de instrucciones SQL para consultar tablas en el metastore. Esto proporciona la flexibilidad de un lago de datos con el esquema de datos estructurado y las consultas basadas en SQL de un almacenamiento de datos relacional, de ah√≠ el t√©rmino "almac√©n de lago de datos".

### Creaci√≥n de una tabla

Las tablas de un metastore de Spark son abstracciones relacionales de los archivos del lago de datos. Las tablas se pueden *administrar* mediante el metastore, o de modo *externo* y se administran independientemente del metastore.

1.  Agrega una nueva celda de c√≥digo al cuaderno y escribe el c√≥digo siguiente, que guarda el DataFrame de los datos de pedidos de ventas en una tabla llamada *salesorders*:

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

>[!NOTE]
> En este ejemplo, no se proporciona ninguna ruta de acceso expl√≠cita, por lo que el metastore administrar√° los archivos de la tabla. Adem√°s, la tabla se guarda en formato Delta, que agrega funcionalidades de base de datos relacionales a tablas. Esto incluye compatibilidad con transacciones, control de versiones de fila y otras caracter√≠sticas √∫tiles. La creaci√≥n de tablas en formato Delta es m√°s conveniente para almacenes de lago de datos de Fabric.

2. Ejecuta la celda de c√≥digo y revisa la salida, que describe la definici√≥n de la nueva tabla.

3. En el panel **Almacenes de lago**, en ... el men√∫ de la carpeta Tablas, selecciona **Actualizar**. A continuaci√≥n, expande el nodo **Tablas** y comprueba que se ha creado la tabla **salesorders**.

![Imagen de pantalla que muestra que se ha creado la tabla salesorders.](Images/salesorders-table.jpg)

4. En ... el men√∫ de la tabla salesorders, selecciona **Cargar datos** > **Spark**. Se agrega una nueva celda de c√≥digo que contiene c√≥digo similar al siguiente:

```pyspark
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

5. Ejecuta el nuevo c√≥digo, que usa la biblioteca de Spark SQL para insertar una consulta SQL en la tabla *salesorder* en c√≥digo de PySpark y cargar los resultados de la consulta en un DataFrame.

### Ejecuci√≥n de c√≥digo SQL en una celda

Aunque resulta √∫til poder insertar instrucciones SQL en una celda que contenga c√≥digo de PySpark, los analistas de datos suelen preferir trabajar directamente en SQL.

1. Agrega una nueva celda de c√≥digo al cuaderno y escribe en ella el c√≥digo siguiente:

```SparkSQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

7. Ejecuta la celda y revisa los resultados. Observa lo siguiente:

* El comando **%%sql** al principio de la celda (denominado magic) cambia el lenguaje a Spark SQL en lugar de PySpark.
* El c√≥digo SQL hace referencia a la tabla *salesorders* que creaste anteriormente.
* La salida de la consulta SQL se muestra autom√°ticamente como resultado en la celda.

>[!NOTE]
> Para obtener m√°s informaci√≥n sobre Spark SQL y los DataFrame, consulta la documentaci√≥n de [Apache Spark SQL](https://spark.apache.org/sql/).

## Visualizaci√≥n de datos con Spark

Los gr√°ficos le ayudan a ver patrones y tendencias m√°s r√°pido de lo que ser√≠a posible mediante el examen de miles de filas de datos. Los cuadernos de Fabric incluyen una vista de gr√°fico integrada, pero no est√° dise√±ada para gr√°ficos complejos. Para obtener m√°s control sobre c√≥mo se crean los gr√°ficos a partir de datos en DataFrames, usa las bibliotecas de gr√°ficos de Python como *matplotlib* o *seaborn*.

### Visualizaci√≥n de los resultados en un gr√°fico

1. Agrega una nueva celda de c√≥digo y escribe el siguiente c√≥digo:

```python
%%sql
SELECT * FROM salesorders
```

2. Ejecuta el c√≥digo para mostrar los datos de la vista salesorders que creaste anteriormente. En la secci√≥n de resultados debajo de la celda, cambia la opci√≥n **Ver** de **Tabla** a **Gr√°fico**.

3.  Usa el bot√≥n **Personalizar gr√°fico** situado en la parte superior derecha del gr√°fico para establecer las siguientes opciones:

* Tipo de gr√°fico:  Gr√°fico de barras.
* Clave: Elemento.
* Valores: Cantidad.
* Grupo de series: d√©jelo en blanco.
* Agregaci√≥n: Suma.
* Apilado: No seleccionado.

Cuando hayas finalizado, selecciona **Aplicar**.

4. El gr√°fico debe tener un aspecto similar al siguiente:

![Imagen de pantalla de la vista de gr√°fico de Cuadernos de Fabric.](Images/built-in-chart.jpg) 

### Introducci√≥n a matplotlib

1. Agrega una nueva celda de c√≥digo y escribe el siguiente c√≥digo:

```python
sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \
            FROM salesorders \
            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
            ORDER BY OrderYear"
df_spark = spark.sql(sqlQuery)
df_spark.show()
```

2. Ejecuta el c√≥digo. Devuelve un DataFrame de Spark que contiene los ingresos anuales. Para visualizar los datos en un gr√°fico, comenzaremos usando la biblioteca matplotlib de Python. Esta biblioteca es la biblioteca de trazado principal en la que se basan muchas otras y proporciona una gran flexibilidad en la creaci√≥n de gr√°ficos.

3. Agrega una nueva celda de c√≥digo y agrega el c√≥digo siguiente:

```python
from matplotlib import pyplot as plt

# matplotlib requires a Pandas dataframe, not a Spark one
df_sales = df_spark.toPandas()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])

# Display the plot
plt.show()
```

4. Ejecuta la celda y revisa los resultados, que constan de un gr√°fico de columnas con los ingresos brutos totales de cada a√±o. Revisa el c√≥digo y observa lo siguiente:

* La biblioteca matplotlib requiere un DataFrame de Pandas, por lo que debes convertir el DataFrame de Spark devuelto por la consulta de Spark SQL.
* En el centro de la biblioteca matplotlib se encuentra el objeto *pyplot*. Esta es la base de la mayor parte de la funcionalidad de trazado.
* La configuraci√≥n predeterminada da como resultado un gr√°fico utilizable, pero hay un margen considerable para personalizarla.

5.  Modifica el c√≥digo para trazar el gr√°fico de la siguiente manera:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

6. Vuelve a ejecutar la celda de c√≥digo y observa los resultados. El gr√°fico ahora es m√°s f√°cil de entender.
7. Un trazado est√° t√©cnicamente contenido con una Figura. En los ejemplos anteriores, la figura se cre√≥ impl√≠citamente; pero puedes crearla expl√≠citamente. Modifica el c√≥digo para trazar el gr√°fico de la siguiente manera:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(8,3))

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

8. Vuelve a ejecutar la celda de c√≥digo y observa los resultados. La figura determina la forma y el tama√±o del trazado.
9. Una figura puede contener varios subtrazados, cada uno en su propio eje. Modifica el c√≥digo para trazar el gr√°fico de la siguiente manera:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a figure for 2 subplots (1 row, 2 columns)
fig, ax = plt.subplots(1, 2, figsize = (10,4))

# Create a bar plot of revenue by year on the first axis
ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
ax[0].set_title('Revenue by Year')

# Create a pie chart of yearly order counts on the second axis
yearly_counts = df_sales['OrderYear'].value_counts()
ax[1].pie(yearly_counts)
ax[1].set_title('Orders per Year')
ax[1].legend(yearly_counts.keys().tolist())

# Add a title to the Figure
fig.suptitle('Sales Data')

# Show the figure
plt.show()
```

10. Vuelve a ejecutar la celda de c√≥digo y observa los resultados. 

>[!NOTE] 
> Para obtener m√°s informaci√≥n sobre el trazado con matplotlib, consulta la documentaci√≥n de [matplotlib](https://matplotlib.org/).

### Uso de la biblioteca seaborn

Aunque *matplotlib* permite crear gr√°ficos complejos de varios tipos, puede que sea necesario c√≥digo complejo para lograr los mejores resultados. Por esta raz√≥n, se han creado muchas bibliotecas nuevas sobre la base de matplotlib para abstraer su complejidad y mejorar sus capacidades. Una de estas bibliotecas es seaborn.

1. Agrega una nueva celda de c√≥digo al cuaderno y escribe en ella el c√≥digo siguiente: 

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

2. Ejecuta el c√≥digo para mostrar un gr√°fico de barras mediante la biblioteca seaborn.
3. Modifica el c√≥digo de la siguiente manera:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Set the visual theme for seaborn
sns.set_theme(style="whitegrid")

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

4.  Ejecuta el c√≥digo modificado y observa que seaborn te permite establecer un tema de color coherente para tus trazados.
5.  Vuelve a modificar el c√≥digo de la siguiente manera:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a line chart
ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

6.  Ejecuta el c√≥digo modificado para ver los ingresos anuales en gr√°fico de l√≠neas.

>[!NOTE]
> Para obtener m√°s informaci√≥n sobre el trazado con seaborn, consulta la documentaci√≥n de [seaborn](https://seaborn.pydata.org/index.html).

## Limpieza de recursos

En este ejercicio, has aprendido a usar Spark para trabajar con datos en Microsoft Fabric.

Si has terminado de explorar los datos, puedes terminar la sesi√≥n de Spark y eliminar el √°rea de trabajo que has creado para este ejercicio.

1.  En el men√∫ del cuaderno, selecciona **Detener sesi√≥n** para finalizar la sesi√≥n con Spark.
1.  En la barra de la izquierda, selecciona el icono del √°rea de trabajo para ver todos los elementos que contiene.
1.  Selecciona **Configuraci√≥n del √°rea de trabajo** y, en la secci√≥n **General**, despl√°zate hacia abajo y selecciona **Quitar esta √°rea de trabajo**.
1.  Selecciona **Eliminar** para eliminar el √°rea de trabajo.

